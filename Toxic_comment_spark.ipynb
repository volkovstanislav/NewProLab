{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bd-master.newprolab.com:4054\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f06e4eb0cc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"comment_text\", StringType()),\n",
    "    StructField(\"toxic\", IntegerType()),\n",
    "    StructField(\"severe_toxic\", IntegerType()),\n",
    "    StructField(\"obscene\", IntegerType()),\n",
    "    StructField(\"threat\", IntegerType()),\n",
    "    StructField(\"insult\", IntegerType()),\n",
    "    StructField(\"identity_hate\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n",
      "\"0000997932d777bf\",\"Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",0,0,0,0,0,0\n",
      "\"000103f0d9cfb60f\",\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",0,0,0,0,0,0\n",
      "\"000113f07ec002fd\",\"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",0,0,0,0,0,0\n",
      "\"0001b41b1c6bb37e\",\"\"\"\n",
      "More\n",
      "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"\"\"types of accidents\"\"\"\"  -I think the references may need tidying so"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/train.csv\",\n",
    "                         schema=schema, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|              id|\n",
      "+----------------+\n",
      "|0000997932d777bf|\n",
      "|000103f0d9cfb60f|\n",
      "|000113f07ec002fd|\n",
      "|0001b41b1c6bb37e|\n",
      "|0001d958c54c6e35|\n",
      "|00025465d4725e87|\n",
      "|0002bcb3da6cb337|\n",
      "|00031b1e95af7921|\n",
      "|00037261f536c51d|\n",
      "|00040093b2687caa|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0000997932d777bf                                                                                                                                                                                                                                                         \n",
      " comment_text  | Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27 \n",
      " toxic         | 0                                                                                                                                                                                                                                                                        \n",
      " severe_toxic  | 0                                                                                                                                                                                                                                                                        \n",
      " obscene       | 0                                                                                                                                                                                                                                                                        \n",
      " threat        | 0                                                                                                                                                                                                                                                                        \n",
      " insult        | 0                                                                                                                                                                                                                                                                        \n",
      " identity_hate | 0                                                                                                                                                                                                                                                                        \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000103f0d9cfb60f                                                                                                                                                                                                                                                         \n",
      " comment_text  | D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)                                                                                                                                                         \n",
      " toxic         | 0                                                                                                                                                                                                                                                                        \n",
      " severe_toxic  | 0                                                                                                                                                                                                                                                                        \n",
      " obscene       | 0                                                                                                                                                                                                                                                                        \n",
      " threat        | 0                                                                                                                                                                                                                                                                        \n",
      " insult        | 0                                                                                                                                                                                                                                                                        \n",
      " identity_hate | 0                                                                                                                                                                                                                                                                        \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target (toxic/non-toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = f.when(\n",
    "    (dataset.toxic == 0) &\n",
    "    (dataset.severe_toxic == 0) &\n",
    "    (dataset.obscene == 0) &\n",
    "    (dataset.threat == 0) &\n",
    "    (dataset.insult == 0) &\n",
    "    (dataset.identity_hate == 0),\n",
    "    0\n",
    ").otherwise(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumn(\"target\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|              id|target|\n",
      "+----------------+------+\n",
      "|d7dda6819645adde|     0|\n",
      "|b63771a6dd0bc63c|     0|\n",
      "|50d5173d2859f0f6|     0|\n",
      "|9b37a5f022dbf83d|     0|\n",
      "|1b3a6936a549fb69|     0|\n",
      "|a5500eb5e6fd55e1|     0|\n",
      "|b2b0261c102976ea|     0|\n",
      "|63d49f7a1ffb31e8|     0|\n",
      "|65900680c3e6830c|     0|\n",
      "|30b38494a74d3b93|     0|\n",
      "|ce1e5dea4c45912d|     0|\n",
      "|a82c1a417bc9a127|     0|\n",
      "|cb8c35fe680e2922|     0|\n",
      "|94691a813924fe0f|     0|\n",
      "|f6123cf9383dc725|     1|\n",
      "|80f80c982361b92d|     1|\n",
      "|034f7ba2351c5a38|     0|\n",
      "|6c8ed3b4acadab3c|     0|\n",
      "|d341af87710d4e0c|     0|\n",
      "|ba87d5df3f47429e|     0|\n",
      "+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"id\", \"target\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------\n",
      " id            | 901d7783bca3d63f                                     \n",
      " comment_text  | Use one that doesn't round numbers up then, asshole. \n",
      " toxic         | 1                                                    \n",
      " severe_toxic  | 0                                                    \n",
      " obscene       | 1                                                    \n",
      " threat        | 0                                                    \n",
      " insult        | 1                                                    \n",
      " identity_hate | 0                                                    \n",
      " target        | 1                                                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.filter(f.col(\"id\") == \"901d7783bca3d63f\").show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=1, count=16225), Row(target=0, count=143346)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupBy(\"target\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10167887648758234"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16225 / (16225 + 143346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id           | 6fdb7b6734f8bf40                                                                                                                                                                                                                                                                                                                    \n",
      " comment_text | \"\n",
      "\n",
      "\"\"Katara\"\"\n",
      "I've removed the section entirely. I don't care if you like to pretend that Katara and Zuko are meant for each other. It's still not case, and there has been no indication whatsoever. Thus, there's little point to actually have the section and exempt Toph and Sokka beyond the insane delusions of shippers.  \" \n",
      " target       | 0                                                                                                                                                                                                                                                                                                                                   \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id           | 26e1b63617df36b1                                                                                                                                                                                                                                                                                                                    \n",
      " comment_text | \"\n",
      "\n",
      " charlie wilson \n",
      "\n",
      "i didnt notice the music genres that were reverted. However my intention was to revert his alias that you deleted.  His alias a.k.a is actually \"\"Uncle Charlie\"\" and needs to be put back and shouldn't  have been removed.\"                                                                                  \n",
      " target       | 0                                                                                                                                                                                                                                                                                                                                   \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.write.parquet(\"/user/stanislav.volkov/dataset\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tokenizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id    | 6fdb7b6734f8bf40                                                                                                                                                                                                                                                                                                                                                                               \n",
      " words | [\", , \"\"katara\"\", i've, removed, the, section, entirely., i, don't, care, if, you, like, to, pretend, that, katara, and, zuko, are, meant, for, each, other., it's, still, not, case,, and, there, has, been, no, indication, whatsoever., thus,, there's, little, point, to, actually, have, the, section, and, exempt, toph, and, sokka, beyond, the, insane, delusions, of, shippers., , \"] \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id    | 26e1b63617df36b1                                                                                                                                                                                                                                                                                                                                                                               \n",
      " words | [\", , , charlie, wilson, , , i, didnt, notice, the, music, genres, that, were, reverted., however, my, intention, was, to, revert, his, alias, that, you, deleted., , his, alias, a.k.a, is, actually, \"\"uncle, charlie\"\", and, needs, to, be, put, back, and, shouldn't, , have, been, removed.\"]                                                                                             \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2.select(\"id\", \"words\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', comment_text='\"\\n\\n\"\"Katara\"\"\\nI\\'ve removed the section entirely. I don\\'t care if you like to pretend that Katara and Zuko are meant for each other. It\\'s still not case, and there has been no indication whatsoever. Thus, there\\'s little point to actually have the section and exempt Toph and Sokka beyond the insane delusions of shippers.  \"', target=0, words=['\"', '', '\"\"katara\"\"', \"i've\", 'removed', 'the', 'section', 'entirely.', 'i', \"don't\", 'care', 'if', 'you', 'like', 'to', 'pretend', 'that', 'katara', 'and', 'zuko', 'are', 'meant', 'for', 'each', 'other.', \"it's\", 'still', 'not', 'case,', 'and', 'there', 'has', 'been', 'no', 'indication', 'whatsoever.', 'thus,', \"there's\", 'little', 'point', 'to', 'actually', 'have', 'the', 'section', 'and', 'exempt', 'toph', 'and', 'sokka', 'beyond', 'the', 'insane', 'delusions', 'of', 'shippers.', '', '\"'])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset2.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset2.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset2.take(1)[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " '',\n",
       " '\"\"katara\"\"',\n",
       " \"i've\",\n",
       " 'removed',\n",
       " 'the',\n",
       " 'section',\n",
       " 'entirely.',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'to',\n",
       " 'pretend',\n",
       " 'that',\n",
       " 'katara',\n",
       " 'and',\n",
       " 'zuko',\n",
       " 'are',\n",
       " 'meant',\n",
       " 'for',\n",
       " 'each',\n",
       " 'other.',\n",
       " \"it's\",\n",
       " 'still',\n",
       " 'not',\n",
       " 'case,',\n",
       " 'and',\n",
       " 'there',\n",
       " 'has',\n",
       " 'been',\n",
       " 'no',\n",
       " 'indication',\n",
       " 'whatsoever.',\n",
       " 'thus,',\n",
       " \"there's\",\n",
       " 'little',\n",
       " 'point',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'have',\n",
       " 'the',\n",
       " 'section',\n",
       " 'and',\n",
       " 'exempt',\n",
       " 'toph',\n",
       " 'and',\n",
       " 'sokka',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'insane',\n",
       " 'delusions',\n",
       " 'of',\n",
       " 'shippers.',\n",
       " '',\n",
       " '\"']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.take(1)[0].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=100, binary=True, inputCol=tokenizer.getOutputCol(), outputCol=\"word_vector\")\n",
    "dataset2 = hasher.transform(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id          | 6fdb7b6734f8bf40                                                                                                                                                                                                                                                                 \n",
      " word_vector | (100,[0,3,5,10,12,13,18,19,20,25,26,28,29,30,31,33,35,36,37,38,42,43,46,47,58,59,60,63,70,72,80,82,85,88,91,96,98,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id          | 26e1b63617df36b1                                                                                                                                                                                                                                                                 \n",
      " word_vector | (100,[9,10,12,16,17,18,19,23,25,29,30,33,34,35,40,42,47,56,59,60,62,66,72,73,79,80,81,87,88,95,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2.select(\"id\", \"word_vector\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher_freq = HashingTF(numFeatures=100, binary=False, inputCol=tokenizer.getOutputCol(), outputCol=\"word_vector_freq\")\n",
    "dataset2_freq = hasher_freq.transform(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id               | 6fdb7b6734f8bf40                                                                                                                                                                                                                                                                 \n",
      " word_vector_freq | (100,[0,3,5,10,12,13,18,19,20,25,26,28,29,30,31,33,35,36,37,38,42,43,46,47,58,59,60,63,70,72,80,82,85,88,91,96,98,99],[2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0]) \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id               | 26e1b63617df36b1                                                                                                                                                                                                                                                                 \n",
      " word_vector_freq | (100,[9,10,12,16,17,18,19,23,25,29,30,33,34,35,40,42,47,56,59,60,62,66,72,73,79,80,81,87,88,95,99],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,7.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,2.0])                                                \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2_freq.select(\"id\", \"word_vector_freq\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset2.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=1, count=12906), Row(target=0, count=114769)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"target\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10143668036797808"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12956 / (12956 + 114769)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset2.join(train, on=\"id\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target=1, count=3319), Row(target=0, count=28577)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby(\"target\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10405693503887635"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3319 / (3319 + 28577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, target: int, words: array<string>, word_vector: vector]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(\"comment_text\", \"words\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(\"comment_text\", \"words\").coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=hasher.getOutputCol(), labelCol=\"target\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_43aa6ddceed6, numClasses = 2, numFeatures = 100"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, target: int, word_vector: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------\n",
      " id            | 00b4fb897de56d6d                          \n",
      " target        | 0                                         \n",
      " prediction    | 0.0                                       \n",
      " probability   | [0.8219752022441963,0.17802479775580368]  \n",
      " rawPrediction | [1.5297873731912435,-1.5297873731912435]  \n",
      "-RECORD 1--------------------------------------------------\n",
      " id            | 016f929ca882f152                          \n",
      " target        | 0                                         \n",
      " prediction    | 0.0                                       \n",
      " probability   | [0.9682786314416439,0.031721368558356056] \n",
      " rawPrediction | [3.4185293474532754,-3.4185293474532754]  \n",
      "-RECORD 2--------------------------------------------------\n",
      " id            | 01f29c98adb73328                          \n",
      " target        | 0                                         \n",
      " prediction    | 0.0                                       \n",
      " probability   | [0.9283264668215503,0.07167353317844981]  \n",
      " rawPrediction | [2.5612619204021385,-2.5612619204021385]  \n",
      "-RECORD 3--------------------------------------------------\n",
      " id            | 03effbaf048d353d                          \n",
      " target        | 0                                         \n",
      " prediction    | 0.0                                       \n",
      " probability   | [0.9036107977704729,0.096389202229527]    \n",
      " rawPrediction | [2.2380045489705465,-2.2380045489705465]  \n",
      "-RECORD 4--------------------------------------------------\n",
      " id            | 03fd85f60e065724                          \n",
      " target        | 1                                         \n",
      " prediction    | 0.0                                       \n",
      " probability   | [0.6049370105031098,0.3950629894968902]   \n",
      " rawPrediction | [0.426079118471286,-0.426079118471286]    \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"id\", \"target\", \"prediction\", \"probability\", \"rawPrediction\").show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28612"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"target\", f.col(\"prediction\").cast(\"int\")).filter(\"target == prediction\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31896"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8970403812390269\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is {}\".format(28612 / 31896))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"target\", f.col(\"prediction\").cast(\"int\"))\\\n",
    "           .filter((f.col(\"target\") == 1) & (f.col(\"prediction\") == f.col(\"target\")))\\\n",
    "           .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pd = predictions.select(\"target\", f.col(\"prediction\").cast(\"int\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  prediction\n",
       "0       0           0\n",
       "1       0           0\n",
       "2       0           0\n",
       "3       0           0\n",
       "4       1           0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.getOrDefault(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     28577\n",
      "           1       0.58      0.04      0.07      3319\n",
      "\n",
      "    accuracy                           0.90     31896\n",
      "   macro avg       0.74      0.52      0.51     31896\n",
      "weighted avg       0.87      0.90      0.85     31896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions_pd.target, predictions_pd.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"target\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8970403812390269"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluator.setMetricName(\"weightedPrecision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662299348750517"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluator.setMetricName(\"weightedRecall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8970403812390269"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap on pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.parquet(\"/data/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, target: int]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    hasher,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.join(train, on=\"id\", how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_1c3cdd58b818"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"target\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.517563489329928"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol=hasher.getOutputCol(), labelCol=\"target\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    hasher,\n",
    "    gbt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.503875266050001"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=1000, binary=True, inputCol=swr.getOutputCol(), outputCol=\"word_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    hasher,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_1754c9eb1a31,\n",
       " StopWordsRemover_aff2fe31c015,\n",
       " HashingTF_e8c73cda5907,\n",
       " LogisticRegressionModel: uid = LogisticRegression_43aa6ddceed6, numClasses = 2, numFeatures = 1000]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6421312094465557"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featute Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumn(\"comment_length\", f.length(dataset.comment_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}).cache()\n",
    "test = dataset.join(train, on=\"id\", how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, target: int, comment_length: int]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[hasher.getOutputCol(), \"comment_length\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"target\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    hasher,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_1754c9eb1a31,\n",
       " StopWordsRemover_aff2fe31c015,\n",
       " HashingTF_e8c73cda5907,\n",
       " VectorAssembler_c0d450754c81,\n",
       " LogisticRegressionModel: uid = LogisticRegression_3f0d96417684, numClasses = 2, numFeatures = 1001]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6483890430396612"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007232870033029963"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages[-1].coefficients[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В любом случае, у нас еще не иссякли идеи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[count_vectorizer.getOutputCol(), \"comment_length\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    count_vectorizer,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408287713365735"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(count_vectorizer.vocabSize, [100, 500])\\\n",
    "                              .addGrid(lr.regParam, [0.01, 0.05])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='CountVectorizer_f03f497725c8', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 100,\n",
       "  Param(parent='LogisticRegression_3f0d96417684', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='CountVectorizer_f03f497725c8', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 100,\n",
       "  Param(parent='LogisticRegression_3f0d96417684', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='CountVectorizer_f03f497725c8', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 500,\n",
       "  Param(parent='LogisticRegression_3f0d96417684', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='CountVectorizer_f03f497725c8', name='vocabSize', doc='max size of the vocabulary. Default 1 << 18.'): 500,\n",
       "  Param(parent='LogisticRegression_3f0d96417684', name='regParam', doc='regularization parameter (>= 0).'): 0.05}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator, numFolds=3, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5752475340153035, 0.5707769791166641, 0.67927483923572, 0.662005513278917]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_2c47c95b27cc"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849586581399666"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
